# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colaboratory.


"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/AI'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense,Dropout,Activation,Flatten,LeakyReLU
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
from keras.initializers import RandomNormal
from keras.regularizers import l1

"""
# ساخت دیتاست جدید خطی



"""

new_file = open('data.csv', 'w')

for i in range(10000):
    weights = np.array([-1.7, 1.35, 8.75]).reshape((1, 3))
    x = np.random.randn(3) * 5
    y = np.matmul(weights, x) + 17.76
    new_file.write(
        ','.join(map(str, list(x) + list(y)))
    )
    new_file.write('\n')

new_file.close()

dataframe = pd.read_csv("data.csv")

nn_input = np.asarray(dataframe.iloc[:,0:dataframe.shape[1]-1])
nn_output = np.asarray(dataframe.iloc[:,-1])

from google.colab import drive
drive.mount('/content/drive')

"""# تقسیم بندی داده ها به سه دسته ی مستقل آموزشی و ارزیاب و تست

"""

from sklearn.model_selection import train_test_split 
x_train, x_temp, y_train, y_temp = train_test_split( nn_input, nn_output, test_size = 0.1, random_state = 55) 
x_valid, x_test, y_valid, y_test = train_test_split( x_temp, y_temp, test_size = 0.1, random_state = 65)

x_train.shape, x_valid.shape, x_test.shape

"""# <center dir="rtl"> مدل اول </center>

<div dir="rtl">

در این مدل شبکه ای پیاده سازی شده است که سه لایه دارد، لایه ی اول شامل ۳۰ نرون، لایه ی دوم شامل ۵۰ نرون و لایه ی آخر که خروجی است یک نرون دارد


استفاده شده است MSE Lossدر این شبکه از

بعنوان تابع بهینه ساز استفاده شده است Adamهمچنین از تابع 


:نتیجه

عملکرد این مدل را بررسی  MAE ,MSE با استفاده از معیارهای epochs بعد از آموزش شبکه به اندازه ی ۱۰۰ 
می کنیم


MSE = 0.668      ,     MAE = 0.035
</div>

"""

model1 = Sequential()

model1.add(Dense(30, input_dim=dataframe.shape[1]-1))
model1.add(Dense(50))
model1.add(Dense(1))

model1.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae', 'mape'])

history = model1.fit(x_test, y_test, epochs=100, batch_size=len(x_test), verbose=1)

predictions = model1.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""# <center dir="rtl"> مدل دوم </center>

<div dir="rtl">

در این مدل شبکه ای پیاده سازی شده است که ۷ لایه دارد و هز لایه به ترتیب ۲۰۰، ۵۰۰، ۸۰۰، ۹۰۰، ۵۰۰،، ۱۰۰ و
لایه ی آخر یک نرون دارد

بعنوان تابع فعال ساز در همه ی لایه ها از Relu استفاده کرده ایم  


در این شبکه از تابع بهینه ساز Adagrad 
استفاده میکنیم 

همچنین مدل ۲۰۰ ایپاک آموزش داده  شده است

در این شبکه از تابع بهینه ساز MSE استفاده شده است

:نتیجه

MSE = 0.665     ,      MAE = 0.0008

</div>

"""

model2 = Sequential()
model2.add(Dense(200, input_dim=dataframe.shape[1]-1, activity_regularizer=l1(0.005)))
model2.add(Activation('relu'))
model2.add(Dense(500, activity_regularizer=l1(0.006)))
model2.add(Activation('relu'))
model2.add(Dense(800, activity_regularizer=l1(0.004)))
model2.add(Activation('relu'))
model2.add(Dense(900, activity_regularizer=l1(0.004)))
model2.add(Activation('relu'))
model2.add(Dense(500, activity_regularizer=l1(0.004)))
model2.add(Activation('relu'))
model2.add(Dense(100, activity_regularizer=l1(0.004)))
model2.add(Activation('relu'))
model2.add(Dense(1, activity_regularizer=l1(0.002)))

model2.compile(loss='mse',
                  optimizer='adagrad',
                  metrics=['mse', 'mae', 'mape'])

history = model2.fit(x_train, y_train, epochs = 200,
                        validation_data=(x_valid, y_valid),
                        batch_size = 16, verbose = 1)

predictions = model2.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""## <center dir="rtl"> تحلیل عملکرد مدل دوم روی داده های زمان آموزش و تست </center>

<div dir="rtl">

با بررسی معیارهای MSE و MAE
روی داده های آموزشی میبینیم که این دو معیار بسیار به صفر نزدیک بودند و 
آنچه که در زمان تست بدست آوردیم بسیار تفاوت دارند ازین رو متوجه میشویم که مدل آورفیت شده و مشکل بیش برازش دارد
علت این مشکل، پیچیده شدن بیش از حد شبکه است بطوری که میتوان گفت شبکه توانسته داده های آموزشی را حفظ میکند اما در زمان مواجهه با داده های جدید با شکست روبرو شده است. در واقع ما در این حالت نتوانسته این بخوبی از امکانات شبکه استفاده کنیم

برای رفع این مشکل شبکه سوم را طراحی میکنیم

</div>

## <center dir="rtl"> مدل سوم </center>

<div dir="rtl">

:شبکه قبلی دچار مشکل بیش برازش شده بود برای رفع این مشکل از دو راه حل استفاده کرده ایم

استفاده از Drop out برای منظم سازی

استفاده از تابع بهینه ساز
rmsprop
که برای مسئله رگرسیون تابع بهینه ساز مناسب تری است

دیگر ویژگیهای این شبکه تماما مثل شبکه قبلی می باشد

نتیجه:

MSE = 0.558       ,       MAE = 0.03

 تغییرات اعمال شده موجب به رفع مشکل بیش برازش شده و مدل بهبود پیدا کرده است




</div>
"""

model3 = Sequential()
model3.add(Dense(200, input_dim=dataframe.shape[1]-1, activity_regularizer=l1(0.005)))
model3.add(Activation('relu'))
model3.add(Dropout(0.1))
model3.add(Dense(500, activity_regularizer=l1(0.006)))
model3.add(Activation('relu'))
model3.add(Dropout(0.2))
model3.add(Dense(800, activity_regularizer=l1(0.004)))
model3.add(Activation('relu'))
model3.add(Dropout(0.3))
model3.add(Dense(900, activity_regularizer=l1(0.004)))
model3.add(Activation('relu'))
model3.add(Dropout(0.3))
model3.add(Dense(500, activity_regularizer=l1(0.004)))
model3.add(Activation('relu'))
model3.add(Dropout(0.2))
model3.add(Dense(100, activity_regularizer=l1(0.004)))
model3.add(Activation('relu'))
model3.add(Dropout(0.4))
model3.add(Dense(1, activity_regularizer=l1(0.002)))

model3.compile(loss='mse',
                  optimizer='rmsprop',
                  metrics=['mse', 'mae', 'mape'])

history = model3.fit(x_train, y_train, epochs = 200,
                        validation_data=(x_valid, y_valid),
                        batch_size = 16, verbose = 1)

predictions = model3.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""## <center dir="rtl"> مدل چهارم </center>

<div dir="rtl">

حالا که با افزودن
Drop out
به شبکه و همچنین تغییر تابع بهینه ساز توانستیم بر بیش برازش چیره شویم امتحان
میکنیم که آیا افزایش میزان
Drop out
نرونها به شبکه قبلی، آیا موجب بهبود کارائی میشود یا خیر



نتیجه:

MSE = 0.545        ,         MAE = 0.042

همانطور که مشخص است این تغییرات تاثیر مثبتی داشته و موجب به بهبود عملکرد مدل شده است

</div>

"""

model4 = Sequential()
model4.add(Dense(200, input_dim=dataframe.shape[1]-1, activity_regularizer=l1(0.005)))
model4.add(Activation('relu'))
model4.add(Dropout(0.2))
model4.add(Dense(500, activity_regularizer=l1(0.006)))
model4.add(Activation('relu'))
model4.add(Dropout(0.3))
model4.add(Dense(800, activity_regularizer=l1(0.004)))
model4.add(Activation('relu'))
model4.add(Dropout(0.4))
model4.add(Dense(900, activity_regularizer=l1(0.004)))
model4.add(Activation('relu'))
model4.add(Dropout(0.5))
model4.add(Dense(500, activity_regularizer=l1(0.004)))
model4.add(Activation('relu'))
model4.add(Dropout(0.4))
model4.add(Dense(100, activity_regularizer=l1(0.004)))
model4.add(Activation('relu'))
model4.add(Dropout(0.3))
model4.add(Dense(1, activity_regularizer=l1(0.002)))

model4.compile(loss='mse',
                  optimizer='rmsprop',
                  metrics=['mse', 'mae', 'mape'])

history = model4.fit(x_train, y_train, epochs = 200,
                        validation_data=(x_valid, y_valid),
                        batch_size = 16, verbose = 1)

predictions = model4.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""## <center dir="rtl"> مدل پنجم </center>

<div dir="rtl">
این مدل تماما مشابه با مدل قبلی ایست با این تفاوت که بجای استفاده از
MSE
بعنوان تابع خطا از
MAE
استفاده شده است


نتیجه:

MSE = 0.515         ,         MAE = 0.020

همانگونه که انتظار میرود، استفاده از این تابع موجب بهبود معیار
MAE
میشود اما 
MSE
بهبودی نداشته است
</div>

"""

model5 = Sequential()
model5.add(Dense(200, input_dim=dataframe.shape[1]-1, activity_regularizer=l1(0.005)))
model5.add(Activation('relu'))
model5.add(Dropout(0.2))
model5.add(Dense(500, activity_regularizer=l1(0.006)))
model5.add(Activation('relu'))
model5.add(Dropout(0.3))
model5.add(Dense(800, activity_regularizer=l1(0.004)))
model5.add(Activation('relu'))
model5.add(Dropout(0.4))
model5.add(Dense(900, activity_regularizer=l1(0.004)))
model5.add(Activation('relu'))
model5.add(Dropout(0.5))
model5.add(Dense(500, activity_regularizer=l1(0.004)))
model5.add(Activation('relu'))
model5.add(Dropout(0.4))
model5.add(Dense(100, activity_regularizer=l1(0.004)))
model5.add(Activation('relu'))
model5.add(Dropout(0.3))
model5.add(Dense(1, activity_regularizer=l1(0.002)))

model5.compile(loss='mae',
                  optimizer='rmsprop',
                  metrics=['mse', 'mae', 'mape'])

history = model5.fit(x_train, y_train, epochs = 200,
                        validation_data=(x_valid, y_valid),
                        batch_size = 16, verbose = 1)

predictions = model5.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""## <center dir="rtl"> مدل ششم </center>

<div dir="rtl">

ساختار اصلی شبکه در این مدل همانند شبکه قبلی است اما در آن از تابع خطای
MAPE
 استفاده شده است. همچنین
ایپاک ها برابر با ۱۰۰۰۰۰۰۰ ست شده است اما به کمک تکنیک 
 early stopping
تنها تا زمانی فرآیند آموزش 
ادامه می یابد که مدل دچار بیش برازش نشده است

نتیجه:

فرآیند آموزش تا ۲۱۷ ایپاک جلو رفت و سپس به پایان رسید

MSE = 0.516       ,        MAE = 0.018

به کمک این تغییرات مدل مجددا بهبود یافته است و
MAE
بدست آمده دچار کاهش شده است
</div>

"""

from keras.callbacks import EarlyStopping

model6 = Sequential()
model6.add(Dense(200, input_dim=dataframe.shape[1]-1, activity_regularizer=l1(0.005)))
model6.add(Activation('relu'))
model6.add(Dropout(0.2))
model6.add(Dense(500, activity_regularizer=l1(0.006)))
model6.add(Activation('relu'))
model6.add(Dropout(0.3))
model6.add(Dense(800, activity_regularizer=l1(0.004)))
model6.add(Activation('relu'))
model6.add(Dropout(0.4))
model6.add(Dense(900, activity_regularizer=l1(0.004)))
model6.add(Activation('relu'))
model6.add(Dropout(0.5))
model6.add(Dense(500, activity_regularizer=l1(0.004)))
model6.add(Activation('relu'))
model6.add(Dropout(0.4))
model6.add(Dense(100, activity_regularizer=l1(0.004)))
model6.add(Activation('relu'))
model6.add(Dropout(0.3))
model6.add(Dense(1, activity_regularizer=l1(0.002)))

model3.compile(loss='mape', metrics=['mse', 'mae', 'mape'], optimizer=Adam(lr=1e-3, decay=1e-3 / 200))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)

history = model3.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=10000000, batch_size=100, verbose=2, callbacks=[es])

predictions = model3.predict(x_test)
print("\n___________________________________________________________")
MSE = np.sum(np.sqrt(np.mean(np.square(predictions - y_test))))/y_test.shape[0]
MAE = np.sum(np.sqrt(np.abs(np.mean(predictions - y_test))))/y_test.shape[0]
print("MSE: ", MSE)
print("MAE: ", MAE)  

plt.plot(history.history['loss'])
plt.plot(history.history['mse'])
plt.plot(history.history['mae'])
plt.plot(history.history['mape'])
plt.show()

"""## <center dir="rtl"> نتیجه گیری نهائی </center>

<div dir="rtl">

با بررسی مدلهای گوناگون به این نتیجه رسیدیم که بهترین تابع بهینه ساز در خصوص این مسئله تابع
rmsprop
میباشد

همچنین بسته به اینکه معیار سنجش کارایی مدل ما، چه چیزی است براساس آن تابع خطا را مشخص میکنیم که در حین آموزش همان معیار بهینه سازی شود

از آنجا که نمیدانیم چه تعداد ایپاک برای آموزش شبکه مناسب است از تکنیک
early stopping
استفاده کرده و
و مدل را با تعداد مناسبی ایپاک آموزش میدهیم


اگر از شبکه ساده ای برای آموزش مدل استفاده کنیم از امکانات گسترده شبکه عصبی بی بهره بودیم و اگر از شبکه ای پیچیده به تنهایی استفاده کنیم باز هم شبکه بر روی داده هایی که قبلا مشاهده نکرده عملکرد خوبی ندارد، به همین علت شبکه ای پیچیده میسازیم و با استفاده از تکنیکهای مختلف منظم سازی از آورفیت شدن مدل جلوگیری میکنیم
از جمله این تکنیک ها در مسئله ی رگرسیون میتوان به تکنیک
Drop out
اشاره کرد

</div>

"""
